{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "In __Neural Net__'s tutorial we saw that the network tries to predict the correct label corresponding to the input data. We saw that for MNIST dataset (which is a dataset of handwritten digits) we tried to predict the correct digit in the image. This type of machine learning algorithm is called __supervised learning__, simply because we are using __labels__.\n",
    "\n",
    "__Autoencoder__ is neural networks that tries to __reconstruct the input data__. Since in training an __Autoencoder__ there are no labels involved, we have an __unsupervised learning__ method. __Autoencoders__ are mainly used for __Dimensionality reduction__. By encoding the input data to a new space (which we usually call ___latent space__) we will have a new representation of the data which is __compressed__ (so it uses less memory), __more abstract__ (contains more essential information of data). \n",
    "\n",
    "__Autoencoder__ help us dealing with noisy data. Since the __latent space__ only keeps the important information, the noise will not be preserved in the space and we can reconstruct the cleaned data.\n",
    "\n",
    "__Autoencoder__ also helps us to understand how the neural networks work. We can visualize what a node has been experted on. This will give us an intuitive about the way these networks perform.\n",
    "\n",
    "\n",
    "In this tutorial we will implement:\n",
    "1. Autoencoder for noise removal\n",
    "2. Visualization of a node's activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "We will start with importing the needed libraries for our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data:\n",
    "For this tutorial we use the MNIST dataset. MNIST is a dataset of handwritten digits. If you are into machine learning, you might have heard of this dataset by now. MNIST is kind of benchmark of datasets for deep learning. One other reason that we use the MNIST is that it is easily accesible through Tensorflow. If you want to know more about the MNIST dataset you can check Yann Lecun's website.\n",
    "We can easily import the dataset and see the size of training, test and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(mnist.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(mnist.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(mnist.validation.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters:\n",
    "Hyper-parameters are important parameters which are not learned by the network. So, we have to specify them externally. These parameters are constant and they are not learnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "logs_path = \"./logs/\"  # path to the folder that we want to save the logs for Tensorboard\n",
    "learning_rate = 0.001  # The optimization learning rate\n",
    "epochs = 10  # Total number of training epochs\n",
    "batch_size = 100  # Training batch size\n",
    "display_freq = 100  # Frequency of displaying the training results\n",
    "\n",
    "# Network Parameters\n",
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_h = img_w = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_h * img_w\n",
    "\n",
    "# number of units in the hidden layer\n",
    "h1 = 200\n",
    "\n",
    "# level of the noise in noisy data\n",
    "noise_level = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Graph:\n",
    "Like before, we start by constructing the graph. But, we need to define some functions that we need rapidly in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "\n",
    "\n",
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        in_dim = x.get_shape()[1]\n",
    "        W = weight_variable(name, shape=[in_dim, num_units])\n",
    "        tf.summary.histogram('W', W)\n",
    "        b = bias_variable(name, [num_units])\n",
    "        tf.summary.histogram('b', b)\n",
    "        layer = tf.matmul(x, W)\n",
    "        layer += b\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our helper functions we can create our graph.\n",
    "\n",
    "We we create an __Autoencoder__ with one hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "# Placeholders for inputs (x), outputs(y)\n",
    "with tf.variable_scope('Input'):\n",
    "    x_original = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X_original')\n",
    "    x_noisy = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X_noisy')\n",
    "\n",
    "\n",
    "fc1 = fc_layer(x_noisy, h1, 'Hidden_layer', use_relu=True)\n",
    "out = fc_layer(fc1, img_size_flat, 'Output_layer', use_relu=False)\n",
    "\n",
    "# Define the loss function, optimizer, and accuracy\n",
    "with tf.variable_scope('Train'):\n",
    "    with tf.variable_scope('Loss'):\n",
    "        loss = tf.reduce_mean(tf.losses.mean_squared_error(x_original, out), name='loss')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train:\n",
    "As soon as the graph is created, we can run it on a session.\n",
    "\n",
    "A ```tf.Session()``` is as good as it's runtime. As soon as the cell is run, the session will be ended and we will loose all the information. So. we will define an _InteractiveSession_ to keep the parameters for testing.\n",
    "\n",
    "To write all the summaries on _logs_ folder for Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Reconstruction loss=0.104\n",
      "iter 100:\t Reconstruction loss=0.033\n",
      "iter 200:\t Reconstruction loss=0.027\n",
      "iter 300:\t Reconstruction loss=0.024\n",
      "iter 400:\t Reconstruction loss=0.024\n",
      "iter 500:\t Reconstruction loss=0.022\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 0.023\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Reconstruction loss=0.023\n",
      "iter 100:\t Reconstruction loss=0.022\n",
      "iter 200:\t Reconstruction loss=0.021\n",
      "iter 300:\t Reconstruction loss=0.021\n",
      "iter 400:\t Reconstruction loss=0.020\n",
      "iter 500:\t Reconstruction loss=0.021\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 0.021\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Reconstruction loss=0.022\n",
      "iter 100:\t Reconstruction loss=0.021\n",
      "iter 200:\t Reconstruction loss=0.020\n",
      "iter 300:\t Reconstruction loss=0.020\n",
      "iter 400:\t Reconstruction loss=0.019\n",
      "iter 500:\t Reconstruction loss=0.020\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 0.020\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Reconstruction loss=0.019\n",
      "iter 100:\t Reconstruction loss=0.020\n",
      "iter 200:\t Reconstruction loss=0.022\n",
      "iter 300:\t Reconstruction loss=0.020\n",
      "iter 400:\t Reconstruction loss=0.020\n",
      "iter 500:\t Reconstruction loss=0.020\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 0.020\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Reconstruction loss=0.020\n",
      "iter 100:\t Reconstruction loss=0.020\n",
      "iter 200:\t Reconstruction loss=0.020\n",
      "iter 300:\t Reconstruction loss=0.019\n",
      "iter 400:\t Reconstruction loss=0.020\n",
      "iter 500:\t Reconstruction loss=0.020\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 0.019\n",
      "---------------------------------------------------------\n",
      "Training epoch: 6\n",
      "iter   0:\t Reconstruction loss=0.019\n",
      "iter 100:\t Reconstruction loss=0.018\n",
      "iter 200:\t Reconstruction loss=0.019\n",
      "iter 300:\t Reconstruction loss=0.019\n",
      "iter 400:\t Reconstruction loss=0.019\n",
      "iter 500:\t Reconstruction loss=0.020\n",
      "---------------------------------------------------------\n",
      "Epoch: 6, validation loss: 0.019\n",
      "---------------------------------------------------------\n",
      "Training epoch: 7\n",
      "iter   0:\t Reconstruction loss=0.018\n",
      "iter 100:\t Reconstruction loss=0.019\n",
      "iter 200:\t Reconstruction loss=0.019\n",
      "iter 300:\t Reconstruction loss=0.018\n",
      "iter 400:\t Reconstruction loss=0.019\n",
      "iter 500:\t Reconstruction loss=0.019\n",
      "---------------------------------------------------------\n",
      "Epoch: 7, validation loss: 0.019\n",
      "---------------------------------------------------------\n",
      "Training epoch: 8\n",
      "iter   0:\t Reconstruction loss=0.019\n",
      "iter 100:\t Reconstruction loss=0.018\n",
      "iter 200:\t Reconstruction loss=0.019\n",
      "iter 300:\t Reconstruction loss=0.018\n",
      "iter 400:\t Reconstruction loss=0.018\n",
      "iter 500:\t Reconstruction loss=0.018\n",
      "---------------------------------------------------------\n",
      "Epoch: 8, validation loss: 0.019\n",
      "---------------------------------------------------------\n",
      "Training epoch: 9\n",
      "iter   0:\t Reconstruction loss=0.018\n",
      "iter 100:\t Reconstruction loss=0.019\n",
      "iter 200:\t Reconstruction loss=0.019\n",
      "iter 300:\t Reconstruction loss=0.019\n",
      "iter 400:\t Reconstruction loss=0.018\n",
      "iter 500:\t Reconstruction loss=0.017\n",
      "---------------------------------------------------------\n",
      "Epoch: 9, validation loss: 0.019\n",
      "---------------------------------------------------------\n",
      "Training epoch: 10\n",
      "iter   0:\t Reconstruction loss=0.019\n",
      "iter 100:\t Reconstruction loss=0.018\n",
      "iter 200:\t Reconstruction loss=0.019\n",
      "iter 300:\t Reconstruction loss=0.019\n",
      "iter 400:\t Reconstruction loss=0.017\n",
      "iter 500:\t Reconstruction loss=0.017\n",
      "---------------------------------------------------------\n",
      "Epoch: 10, validation loss: 0.019\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph (session)\n",
    "sess = tf.InteractiveSession() # using InteractiveSession instead of Session to test network in separate cell\n",
    "sess.run(init)\n",
    "train_writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "num_tr_iter = int(mnist.train.num_examples / batch_size)\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    for iteration in range(num_tr_iter):\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        batch_x_noisy = batch_x + noise_level * np.random.normal(loc=0.0, scale=1.0, size=batch_x.shape)\n",
    "\n",
    "        global_step += 1\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {x_original: batch_x, x_noisy: batch_x_noisy}\n",
    "        _, summary_tr = sess.run([optimizer, merged], feed_dict=feed_dict_batch)\n",
    "        train_writer.add_summary(summary_tr, global_step)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "            loss_batch = sess.run(loss,\n",
    "                                  feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Reconstruction loss={1:.3f}\".\n",
    "                  format(iteration, loss_batch))\n",
    "\n",
    "    # Run validation after every epoch\n",
    "    x_valid_original  = mnist.validation.images\n",
    "    x_valid_noisy = x_valid_original + noise_level * np.random.normal(loc=0.0, scale=1.0, size=x_valid_original.shape)\n",
    "\n",
    "\n",
    "    feed_dict_valid = {x_original: x_valid_original, x_noisy: x_valid_noisy}\n",
    "    loss_valid = sess.run(loss, feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.3f}\".\n",
    "          format(epoch + 1, loss_valid))\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test:\n",
    "Now that the model is trained. It is time to test our model.\n",
    "\n",
    "We will define some helper functions to plot the  original input data. Then we will add some noises to our image and we will feed the noisy image to the network and visualize the reconstructed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(original_images, noisy_images, reconstructed_images):\n",
    "    \"\"\"\n",
    "    Create figure of original and reconstructed image.\n",
    "    :param original_image: original images to be plotted, (?, img_h*img_w)\n",
    "    :param noisy_image: original images to be plotted, (?, img_h*img_w)\n",
    "    :param reconstructed_image: reconstructed images to be plotted, (?, img_h*img_w)\n",
    "    \"\"\"\n",
    "    num_images = original_images.shape[0]\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=.1, wspace=0)\n",
    "    \n",
    "    img_h = img_w = np.sqrt(original_images.shape[-1]).astype(int)\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Plot image.\n",
    "        ax[0].imshow(original_images[i].reshape((img_h, img_w)), cmap='gray')\n",
    "        ax[1].imshow(noisy_images[i].reshape((img_h, img_w)), cmap='gray')\n",
    "        ax[2].imshow(reconstructed_images[i].reshape((img_h, img_w)), cmap='gray')\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        for sub_ax in ax:\n",
    "            sub_ax.set_xticks([])\n",
    "            sub_ax.set_yticks([])\n",
    "    \n",
    "    for ax, col in zip(axes[0], [\"Original Image\", \"Noisy Image\", \"Reconstructed Image\"]):\n",
    "        ax.set_title(col)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network after training\n",
    "# Make a noisy image\n",
    "test_samples = 5\n",
    "x_test = mnist.test.images[:test_samples]\n",
    "x_test_noisy = x_test + noise_level * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "# Reconstruct a clean image from noisy image\n",
    "x_reconstruct = sess.run(out, feed_dict={x_noisy: x_test_noisy})\n",
    "# Calculate the loss between reconstructed image and original image\n",
    "loss_test = sess.run(loss, feed_dict={x_original: x_test, x_noisy: x_test_noisy})\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss of original image compared to reconstructed image : {0:.3f}\".format(loss_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Plot original image, noisy image and reconstructed image\n",
    "plot_images(x_test, x_test_noisy, x_reconstruct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we are finished the testing, we will close the session to free the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the session after you are done with testing\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step our coding is done. We can inspect more in our networkusing the __Tensorboard__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir=logs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the generated link in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* [www.tensorflow.com](www.tensorflow.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! If you have any question or doubt, feel free to leave a comment in our [website](http://easy-tensorflow.com/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
