{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorBoard \n",
    "\n",
    "__TensorBoard__ is a visualization software that comes with any standard TensorFlow installation. In Google’s words: “The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we've included a suite of visualization tools called TensorBoard.”\n",
    "\n",
    "TensorFlow programs can range from  very simple to super complex problems (using thousands of computations), and they all have two basic components, Operations and Tensors. As explained in the previous tutorials, the idea is that you create a model that consists of a set of operations, feed data in to the model and the tensors will flow between the operations until you get an output tensor, your result.\n",
    "\n",
    "When fully configures, TensorBoard window will look something like this:\n",
    "\n",
    "<img src=\"files/files/3_1.png\" width=\"500\" height=\"1000\" >\n",
    "\n",
    "___Fig1. ___ TensorBoard appearance\n",
    "\n",
    "\n",
    "TensorBoard was created as a way to help you understand the flow of tensors in your model so that you can debug and optimize it. It is generally used for two main purposes:\n",
    "\n",
    "1. Graph Visualization\n",
    "2. Writing Summaries (or Visualizing Learning)\n",
    "\n",
    "We'll cover this two main usages of TensorBoard in this tutorial. Learning to use TensorBoard early and often will make working with TensorFlow much more enjoyable and productive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gragh Visualizing\n",
    "\n",
    "Will powerful, TensorFlow computation graphs can become extremely complicated. Visualizing the graph can help you understand and debug it. Here's an example of the visualization at work from TensorFlow website.\n",
    "\n",
    "<img src=\"files/files/3_2.gif\" width=\"500\" height=\"1000\" >\n",
    "\n",
    "___Fig2. ___ Visualization of a TensorFlow graph\n",
    "\n",
    "To make our TensorFlow program __TensorBoard-activated__, we need to add a very few lines of code to it. This will export the TensorFlow operations into a file, called __event file__ (or event log file). TensorBoard is able to read this file and give insight into the model graph and its performance.\n",
    "\n",
    "Now let's write a simple TensorFlow program and visualize its computation graph with TensorBoard.\n",
    "\n",
    "### Example 1:\n",
    "Let's create two constants and add them together. Constant tensors can be defined simply by defining their value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create graph\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "c = tf.add(a, b)\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the program with TensorBoard, we need to write log files of the program. To write event files, we first need to create a __writer__ for those logs, using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter([logdir], [graph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where __[logdir]__ is the folder where you want to store those log files. You can choose [logdir] to be something meaningful such as './graphs'. The second argument __[graph]__ is the graph of the program we're working on. There are two ways to get the graph:\n",
    "1. Call the graph using __tf.get_default_graph()__, which returns the default graph of the program\n",
    "2. set it as __sess.graph__ which returns the session's graph (note that this requires us to already have created a session).\n",
    "\n",
    "We'll show both ways in the following example; however, the second way is more common. Either way, make sure to create a writer only after you’ve defined your graph. Otherwise, the graph visualized on TensorBoard would be incomplete.\n",
    "\n",
    "Let's add the writer to the first example and visualize the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create graph\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "c = tf.add(a, b)\n",
    "\n",
    "# creating the writer out of the session\n",
    "# writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # or creating the writer inside the session\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    print(sess.run(c))\n",
    "    # don't forget to close the writer at the end\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you run this code, it creates a directory inside your current directory (beside your Python) code which contains the __event file__.\n",
    "\n",
    "<img src=\"files/files/3_3.png\" width=\"300\" height=\"600\" >\n",
    "\n",
    "___Fig3. ___ Created directory which contains the event file\n",
    "\n",
    "\n",
    "Next, go to Terminal and make sure that the present working directory is the same as where you ran your Python code. For example, here we can switch to the directory using \n",
    "\n",
    "cd ~/Desktop/tensorboard\n",
    "\n",
    "Then run:\n",
    "\n",
    "tensorboard --logdir=\"./graphs\" --port 6006\n",
    "\n",
    "It will generate a link for you. ctrl+left click on that link (or simply copy it into your browser or just open your browser and go to http://localhost:6006/. This will show the TensorBoard page which will look like:\n",
    "\n",
    "<img src=\"files/files/3_4.png\" width=\"500\" height=\"1000\" >\n",
    "\n",
    "___Fig4. ___ TensorBoard page visualizing the graph generated in Example 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a variable.\n",
    "w = tf.Variable(<initial-value>, name=<optional-name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of creating scalar and matrix variables are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Variable(2, name=\"scalar\") \n",
    "m = tf.Variable([[1, 2], [3, 4]], name=\"matrix\") \n",
    "W = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable __W__ defined above will create a matrix with 784 rows and 10 columns which will be initialized by zeros. This can be used as a weight matrix of a feed-forward neural network (or even in a linear regression model) from a layer with 784 neuron to a layer with 10 neuron. We'll see more of this later in this turorial.\n",
    "\n",
    "__*Note:__ we use tf.Variable() with uppercase \"V\", and tf.constant with lowercase \"c\". You don't necessarily need to know the reason, but it's simply because tf.constant is an op, while tf.Variable is a class with multiple ops.\n",
    "\n",
    "__* IMPORTANT Note:__ Calling tf.Variable to create a variable is the old way of creating a variable. TensorFlow recommends to use the wraper __tf.get_variable__ which accepts the name, shape, etc as its arguments as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_variable(name,\n",
    "                shape=None,\n",
    "                dtype=None,\n",
    "                initializer=None,\n",
    "                regularizer=None,\n",
    "                trainable=True,\n",
    "                collections=None,\n",
    "                caching_device=None,\n",
    "                partitioner=None,\n",
    "                validate_shape=True,\n",
    "                use_resource=None,\n",
    "                custom_getter=None,\n",
    "                constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.get_variable(\"scalar\", initializer=tf.constant(2)) \n",
    "m = tf.get_variable(\"matrix\", initializer=tf.constant([[0, 1], [2, 3]]))\n",
    "W = tf.get_variable(\"weight_matrix\", shape=(784, 10), initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Initialize Variables\n",
    "Variables need to be initialized befor being used. To do so, we have to invoke a __variable initializer operation__ and run the operation on the session. This is the easiest way to initialize variables which initializes all variables at once.\n",
    "\n",
    "The following toy example shows how we can add an op to initialize the variables.\n",
    "\n",
    "### Example 3:\n",
    "create two variables and add them together. Then print out their value and the summation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.get_variable(name=\"var_1\", initializer=tf.constant(2))\n",
    "b = tf.get_variable(name=\"var_2\", initializer=tf.constant(3))\n",
    "c = tf.add(a, b, name=\"Add1\")\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # now let's evaluate their value\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__FailedPreconditionError: Attempting to use uninitialized value__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we ran into __FailedPreconditionError: Attempting to use uninitialized value__. This is because we tried to evaluate the variables before initializing them. Let's correct the code by first initializing all variables and then evaluating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# create graph\n",
    "a = tf.get_variable(name=\"A\", initializer=tf.constant(2))\n",
    "b = tf.get_variable(name=\"B\", initializer=tf.constant(3))\n",
    "c = tf.add(a, b, name=\"Add\")\n",
    "# add an Op to initialize global variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # run the variable initializer operation\n",
    "    sess.run(init_op)\n",
    "    # now let's evaluate their value\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the graph and generated variables:\n",
    "<img src=\"files/files/2_3.png\" width=\"1000\" height=\"2000\" >\n",
    "___Fig3. ___ generated graph (Left) and variables (Right)\n",
    "\n",
    "As you can see, two blue boxes are the generated variables (compare them with constant nodes in Fig. 2) which are added together using \"Add\" operation.\n",
    "\n",
    "\n",
    "__*Note:__ Variables are usually used for weights and biases in neural networks.\n",
    "\n",
    "- __weights__ are usually initialized from a normal distribution using `tf.truncated_normal_initializer()`.\n",
    "\n",
    "- __biases__ are usually initialized from zeros using `tf.zeros_initializer()`.\n",
    "\n",
    "Let's look at a very simple example of creating weight and bias variables with proper initialization:\n",
    "\n",
    "### Example 4:\n",
    "Create the weight and bias matrices for a fully-connected layer with 2 neuron to another layer with 3 neuron.\n",
    "In this scenario, the weight and bias variables must be of size $[2, 3]$ and 3 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = [[-0.00376599 -0.00506956  0.00082394]\n",
      " [ 0.0016487   0.00981423 -0.00226094]]\n",
      "biases = [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# create graph\n",
    "weights = tf.get_variable(name=\"W\", shape=[2,3], initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "biases = tf.get_variable(name=\"b\", shape=[3], initializer=tf.zeros_initializer())\n",
    "\n",
    "# add an Op to initialize global variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # run the variable initializer\n",
    "    sess.run(init_op)\n",
    "    # now we can run our operations\n",
    "    W, b = sess.run([weights, biases])\n",
    "    print('weights = {}'.format(W))\n",
    "    print('biases = {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Placeholder:\n",
    "Placeholders are nodes whose value is fed in at execution time. If you have inputs to your network that depend on some external data and you don't want your graph to depend on any real value, placeholders are the datatype you need. In fact, you can build the graph without needing the data. Therefore, they don't need any initial value; only a datatype (such as float32) and a tensor shape so the graph still knows what to compute even though it doesn't have any stored values yet.\n",
    "\n",
    "Some examples of creating placeholders are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, shape=[5])\n",
    "b = tf.placeholder(dtype=tf.float32, shape=None, name=None)\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784], name='input')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10], name='label') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple example.\n",
    "\n",
    "### Example 5:\n",
    "Create a constant vector and a placeholder and add them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([5, 5, 5], tf.float32, name='A')\n",
    "b = tf.placeholder(tf.float32, shape=[3], name='B')\n",
    "c = tf.add(a, b, name=\"Add\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "      print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__InvalidArgumentError: You must feed a value for placeholder tensor 'B' with dtype float and shape  $[3]$ error__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, running this code will run into an error. As you might have guessed, it is simply because the placeholder is empty and there is no way to add an empty tensor to a constant tensor. To solve this, we need to feed the input value to tensor \"a\". It can be done by creating a dictionary (\"d\" in the following code) whose key(s) are the placeholders and their values are the desired value to be passed to the placeholder(s), and feeding it to an argument called \"feed_dict\". In our example, say we want to pass $[1, 2, 3]$ to the placeholder; the code needs to be modified as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 7. 8.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([5, 5, 5], tf.float32, name='A')\n",
    "b = tf.placeholder(tf.float32, shape=[3], name='B')\n",
    "c = tf.add(a, b, name=\"Add\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a dictionary:\n",
    "    d = {b: [1, 2, 3]}\n",
    "    # feed it to the placeholder\n",
    "    print(sess.run(c, feed_dict=d)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated graph and variables are as follows:\n",
    "<img src=\"files/files/2_4.png\" width=\"700\" height=\"1400\" >\n",
    "___Fig4. ___ generated graph (Left) and variables (Right)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So far so good?\n",
    "\n",
    "\n",
    "## Creating a Toy Neural Network\n",
    "Now, we have all the required materials to start building a toy feed-forward neural network with one hidden layer with 200 hidden units (neurons). The computational graph in Tensorflow will look like this:\n",
    "\n",
    "<img src=\"files/files/2_5.png\" width=\"300\" height=\"600\" >\n",
    "___Fig5. ___ Schematic of the graph for one layer of the neural network\n",
    "\n",
    "How many operations (or nodes) you see in this graph? Six, right? The three circles (X, W, b) and three rectangles. We'll go through them one by one and will discuss what is the best way to implement it.\n",
    "\n",
    "Let's start with the input, X. This can be an input of any type, such as images, signals, etc. The general approach is to feed all inputs to the network and train the trainable parameters (here, W and b) by backpropagating the error signal. Ideally, you need to feed all inputs together, compute the error, and update the parameters. This process is called \"Gradient Descent\".\n",
    "\n",
    "*Side Note: In real-world problems, we have thousands and millions of inputs which makes gradient descent computationally expensive. That's why we split the input set into several shorter pieces (called mini-batch) of size B (called mini-batch size) inputs, and feed them one by one. This is called \"Stochastic Gradient Descent\". The process of feeding each mini-batch of size B to the network, back-propagating errors, and updating the parameters (weights and biases) is called an iteration.\n",
    "\n",
    "We generally use Placeholders for inputs so that we can build the graph without depending on any real value. The only point is that you need to choose the proper size for the input. Here, we have a feed-forward neural network, and let's assume inputs of size 784 (similar to 28x28 images of MNIST data). The input placeholder can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784], name=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder what is shape=$[None, 784]$?!\n",
    "\n",
    "Well, that's the tricky part! Read the above side note one more time. We need to feed B images of size 784 to the network in each training iteration. So the placeholder needs to be of shape=$[B, 784]$. Defining the placeholder shape as $[None, 784]$ means that you can feed any number of images of size 784 (not B images necessarily). This is especially helpful in the evaluation time where you need to feed all validation or test images to the network and compute the performance on all of them.\n",
    "\n",
    "Enough with the placeholder. Let's continue with the network parameters, W, and b. As explained in the Variable section above, they have to be defined as variables. Since in Tensorflow, gradient updates will be applied to the graph variables, by default. As mentioned, variables need to be initialized.\n",
    "\n",
    "*Note: Generally, weights (W) are initialized randomly, in it's the simplest form from a normal distribution, say normal distribution with zero mean and standard deviation of 0.01. Biases (b) can be initialized as small constant values, such as 0.\n",
    "\n",
    "Since the input dimension is 784 and we have 200 hidden units, the weight matrix will be of size $[784, 200]$. We also need 200 biases, one for each hidden unit. The code will be like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weight matrix initialized randomely from N(0, 0.01)\n",
    "weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "W = tf.get_variable(name=\"Weight\", dtype=tf.float32, shape=[784, 200], initializer=weight_initer)\n",
    "\n",
    "# create bias vector of size 200, all initialized as zero\n",
    "bias_initer =tf.constant(0., shape=[200], dtype=tf.float32)\n",
    "b = tf.get_variable(name=\"Bias\", dtype=tf.float32, initializer=bias_initer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to the rectangle operations. We must multiply input X$_{[None, 784]}$ and weight matrix W$_{[784, 200]}$ which gives a tensor of size $[None, 200]$, then add the bias vector b$_{[200]}$ and eventually pass the final tensor from a ReLU non-linearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MatMul node\n",
    "x_w = tf.matmul(X, W, name=\"MatMul\")\n",
    "# create Add node\n",
    "x_w_b = tf.add(x_w, b, name=\"Add\")\n",
    "# create ReLU node\n",
    "h = tf.nn.relu(x_w_b, name=\"ReLU\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we are all set. The created graph looks like this:\n",
    "<img src=\"files/files/2_6.png\" width=\"400\" height=\"800\" >\n",
    "___Fig6. ___ Data flow graph of the neural network created in Tensorflow\n",
    "\n",
    "But how can you visualize this graph? How did you create this figure?! That's the magic of __Tensorboard__. It's thoroughly explained in our next article.\n",
    "\n",
    "Before closing it, let's run a session on this graph (using 100 images generated by random pixel values) and get the output of hidden units (h). The whole code will be like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tensorflow library\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# create the input placeholder\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784], name=\"X\")\n",
    "weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "\n",
    "# create network parameters\n",
    "W = tf.get_variable(name=\"Weight\", dtype=tf.float32, shape=[784, 200], initializer=weight_initer)\n",
    "bias_initer =tf.constant(0., shape=[200], dtype=tf.float32)\n",
    "b = tf.get_variable(name=\"Bias\", dtype=tf.float32, initializer=bias_initer)\n",
    "\n",
    "# create MatMul node\n",
    "x_w = tf.matmul(X, W, name=\"MatMul\")\n",
    "# create Add node\n",
    "x_w_b = tf.add(x_w, b, name=\"Add\")\n",
    "# create ReLU node\n",
    "h = tf.nn.relu(x_w_b, name=\"ReLU\") \n",
    "\n",
    "# Add an Op to initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # initialize variables\n",
    "    sess.run(init_op)\n",
    "    # create the dictionary:\n",
    "    d = {X: np.random.rand(100, 784)}\n",
    "    # feed it to placeholder a via the dict \n",
    "    print(sess.run(h, feed_dict=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code will print out h$_{[100, 200]}$ which are the outputs of 200 hidden units in response to 100 images; i.e. 200 features extracted from 100 images.\n",
    "\n",
    "We'll continue constructing the loss function and creating the optimizer operations in the next articles. However, we need to learn Tensorboard first to use its amazing features in our neural network code.\n",
    "\n",
    "Thanks for reading! If you have any question or doubt, feel free to leave a comment below. You can also send us feedback through contact us page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
